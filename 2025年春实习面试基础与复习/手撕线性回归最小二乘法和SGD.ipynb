{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\tool\\anaconda\\anaconda_root\\lib\\site-packages (1.23.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 2)  # 生成100个二维特征样本，范围[0,2)\n",
    "y_gt = 4 + 3 * X[:, 0] + 3 * X[:, 1] + np.random.randn(100)  # 生成带噪声的标签\n",
    "y_gt = y_gt.reshape(-1, 1)  # 将y转换为列向量 (100,1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳拟合参数： [[4.11450326]\n",
      " [2.78685022]\n",
      " [3.01478753]]\n"
     ]
    }
   ],
   "source": [
    "#第一种，直接求出W\n",
    "def linear_regression(X, y):\n",
    "    ones = np.ones((X.shape[0], 1))\n",
    "    X_b =np.concatenate([ones,X],axis=1)#添加x0=1的列\n",
    "    theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "    return theta_best\n",
    "theta_best = linear_regression(X, y_gt)\n",
    "print(\"最佳拟合参数：\", theta_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第二种，使用SGD\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1 / (2 * m)) * np.sum(np.square(predictions - y))\n",
    "    return cost\n",
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    J_history = np.zeros(num_iters)\n",
    "    for i in range(num_iters):\n",
    "        predictions = X.dot(theta)\n",
    "        error = np.dot(X.transpose(), (predictions - y))\n",
    "        descent = alpha *(1 /m)* error\n",
    "        theta -= descent\n",
    "        J_history[i] = compute_cost(X, y, theta)\n",
    "    return theta, J_history\n",
    "#初始化参数\n",
    "k = np.random.randn(3, 2)\n",
    "#设置超参数\n",
    "learning_rate = 0.01\n",
    "num_iters = 100\n",
    "#添加x0=1的列\n",
    "ones = np.ones((x.shape[0], 1))\n",
    "X_b = np.concatenate([ones, X], axis=1)\n",
    "#梯度下降\n",
    "k,-= gradient_descent(X_b, y_gt, k, learning_rate, num_iters)\n",
    "print(\"梯度下降后的参数：\"，theta)\n",
    "#print(\"预测值：\"，X_b.dot(theta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.767131440448686\n",
      "0.42685250206308495\n",
      "0.42355147198326115\n",
      "0.42304929059902063\n",
      "0.42297219085609256\n",
      "0.42296035310986296\n",
      "0.4229585355644852\n",
      "0.42295825650197244\n",
      "0.42295821365523706\n",
      "0.4229582070766295\n",
      "梯度下降后的参数： [[4.11444997]\n",
      " [2.78687531]\n",
      " [3.01481066]]\n"
     ]
    }
   ],
   "source": [
    "# 手写# 使用SGD（随机梯度下降）优化线性回归\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 2)  # 生成100个二维特征样本，范围[0,2)\n",
    "y = 4 + 3 * X[:, 0] + 3 * X[:, 1] + np.random.randn(100)  # 生成带噪声的标签\n",
    "y = y.reshape(-1, 1)  # 将y转换为列向量 (100,1)\n",
    "def compute_cost(X, y, k):\n",
    "    \"\"\"计算损失函数\"\"\"\n",
    "    predictions = X.dot(k)# X.dot(theta) 表示特征矩阵 X 和参数向量 theta 的矩阵乘法\n",
    "    # L2损失 \n",
    "    cost = (1/(2*len(y))) * np.sum(np.square(predictions - y))\n",
    "    return cost\n",
    "def compute_cost_L1(X, y, theta):\n",
    "    \"\"\"计算L1损失（平均绝对误差）\"\"\"\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1/m) * np.sum(np.abs(predictions - y))  # L1损失公式\n",
    "    return cost\n",
    "def gradient_descent(X, y_gt, k, learning_rate, num_iters):\n",
    "    \"\"\"执行梯度下降算法\"\"\"\n",
    "    # m = \n",
    "    J_history = np.zeros(num_iters)\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # predictions = \n",
    "        grad = np.dot(X.T, (X.dot(k) - y_gt))  # 计算梯度\n",
    "        descent = learning_rate * (1/len(y_gt)) * grad# 将梯度进行平均化\n",
    "        k -= descent  # 参数更新\n",
    "        loss= compute_cost(X, y_gt, k)  # 记录损失历史\n",
    "        J_history[i] = loss  # 记录损失历史\n",
    "        if i%100==0:\n",
    "            print(loss)\n",
    "        # print()\n",
    "    \n",
    "    return k, J_history\n",
    "\n",
    "# 初始化参数（假设是多元线性回归，y有2个输出）\n",
    "k = np.random.randn(3, 1)  # 修正为(3,1)维度，匹配单输出场景\n",
    "\n",
    "# 设置超参数\n",
    "learning_rate = 0.1  # 调整学习率为更安全的数值\n",
    "num_iters = 1000\n",
    "\n",
    "# 假设X是已有的特征矩阵（需要确保X已经正确定义）\n",
    "# 添加x0=1的列（偏置项）\n",
    "ones = np.ones((X.shape[0], 1))  # 修正X.shape[0]的索引\n",
    "X_b = np.concatenate([ones, X], axis=1)\n",
    "\n",
    "# 执行梯度下降（需要确保y已经正确定义）\n",
    "k, J_history = gradient_descent(X_b, y, k, learning_rate, num_iters)  # 修正返回值和解包\n",
    "print(\"梯度下降后的参数：\", k)  # 展平输出更美观\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手写# 使用SGD（随机梯度下降）优化线性回归\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 2)  # 生成100个二维特征样本，范围[0,2)\n",
    "y = 4 + 3 * X[:, 0] + 3 * X[:, 1] + np.random.randn(100)  # 生成带噪声的标签\n",
    "y = y.reshape(-1, 1)  # 将y转换为列向量 (100,1)\n",
    "y.shape[0],len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m theta=np.random.randn(\u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m) \n\u001b[32m     22\u001b[39m learning_rate=\u001b[32m0.001\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m res=\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# def compute_loss(X,y,theta):\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#     loss_log=np.zeros()\\\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mgradient_descent\u001b[39m\u001b[34m(X_b, y_gt, theta, epoch, lr)\u001b[39m\n\u001b[32m     11\u001b[39m m=y_gt.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# 梯度\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     grad=\u001b[32m1\u001b[39m/m*np.dot(\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,(X*theta-y_gt)) \n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# 参数\u001b[39;00m\n\u001b[32m     16\u001b[39m     delta_para = lr* grad\n",
      "\u001b[31mTypeError\u001b[39m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "b = np.ones((X.shape[0], 1))\n",
    "X_b=np.concatenate([b,X],axis=1)\n",
    "\n",
    "# np.concatenate([ones, X], axis=1)\n",
    "# def compute_loss()\n",
    "def compute_loss(X_b,y_gt,theta):\n",
    "    m=y_gt.shape[0]\n",
    "    loss=1/(2*m)*np.sum(np.square(np.dot(X_b,theta)-y_gt))\n",
    "    \n",
    "def gradient_descent(X_b,y_gt,theta,epoch,lr=0.01):\n",
    "    m=y_gt.shape[0]\n",
    "    for i in range(epoch):\n",
    "        # 梯度\n",
    "        grad=1/m*np.dot(X.T(),(X*theta-y_gt)) \n",
    "        # 参数\n",
    "        delta_para = lr* grad\n",
    "        # params\n",
    "        theta-=delta_para\n",
    "        # loss\n",
    "        loss=compute_loss(X_b,y_gt,theta)\n",
    "theta=np.random.randn(3, 1) \n",
    "learning_rate=0.001\n",
    "res=gradient_descent(X_b, y, theta, epoch=100,lr=0.001) \n",
    "# def compute_loss(X,y,theta):\n",
    "#     loss_log=np.zeros()\\\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
