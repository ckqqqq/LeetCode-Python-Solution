![img](https://ckqqqq-qiker-image-service.oss-cn-beijing.aliyuncs.com/typora-image/%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81.png)

æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRotary Position Embedding, RoPEï¼‰. RoPE å·§å¦™åœ°ä½¿ç”¨äº†åŸºäºç»å¯¹ä½ç½®ä¿¡æ¯çš„æ—‹è½¬çŸ©é˜µæ¥è¡¨ç¤ºæ³¨æ„åŠ›ä¸­çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚

RoPE æ ¹æ®ä½ç½®ä¿¡æ¯ä¸ºåºåˆ—ä¸­æ¯ä¸ªè¯å…ƒæ‰€å¯¹åº”çš„è®¾ç½®äº†ç‹¬æœ‰çš„æ—‹è½¬çŸ©é˜µï¼Œå¹¶å’Œå¯¹åº”çš„æŸ¥è¯¢å’Œé”®è¿›è¡Œç›¸ä¹˜è¿›è¡Œèåˆã€‚

ä½ç½®ç´¢å¼•ä¸º ğ‘¡ å¯¹åº”çš„æ—‹è½¬çŸ©é˜µå®šä¹‰å¦‚ä¸‹æ‰€ç¤ºï¼š
![](https://segmentfault.com/img/remote/1460000045501346)

åˆ©ç”¨æ—‹è½¬çŸ©é˜µä¸­ä¸‰è§’å‡½æ•°çš„ç‰¹æ€§ï¼Œä½ç½®ç´¢å¼•ä¸º ğ‘– çš„æ—‹è½¬çŸ©é˜µå’Œä½ç½®ç´¢å¼•ä¸º ğ‘— çš„æ—‹è½¬çŸ©é˜µçš„è½¬ç½®çš„ä¹˜ç§¯ç­‰åŒäºä½ç½®ç´¢å¼•ä¸ºå®ƒä»¬ç›¸å¯¹è·ç¦»ğ‘–âˆ’ğ‘— çš„æ—‹è½¬çŸ©é˜µï¼Œé€šè¿‡è¿™ç§æ–¹å¼ï¼Œé”®å’ŒæŸ¥è¯¢ä¹‹é—´çš„æ³¨æ„åŠ›åˆ†æ•°èƒ½å¤Ÿæœ‰æ•ˆèå…¥ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œå› ä¸ºQã€Kçš„æ³¨æ„åŠ›è®¡ç®—ä¸­åªä¸ğ‘–ã€ğ‘—ç›¸å¯¹ä½ç½®æœ‰å…³ã€‚

æ³¨æ„åŠ›è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š
![](https://segmentfault.com/img/remote/1460000045501347)

å¤§æ¨¡å‹åœ¨ä½ç½®ç¼–ç ä¸Šï¼Œä½¿ç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRotary Positional Embeddingsï¼ŒRoPEï¼‰ä»£æ›¿åŸæœ‰çš„ç»å¯¹ä½ç½®ç¼–ç ã€‚

æ—‹è½¬ä½ç½®ç¼–ç RoPEæ˜¯ä¸€ç§å›ºå®šå¼çš„ç»å¯¹ä½ç½®ç¼–ç ç­–ç•¥ï¼Œä½†æ˜¯å®ƒçš„ç»å¯¹ä½ç½®ç¼–ç é…åˆTransformerçš„Attentionå†…ç§¯æ³¨æ„åŠ›æœºåˆ¶èƒ½è¾¾åˆ°ç›¸å¯¹ä½ç½®ç¼–ç çš„æ•ˆæœã€‚

RoPEçš„æœ¬è´¨æ˜¯å¯¹ä¸¤ä¸ªtokenå½¢æˆçš„Queryå’ŒKeyå‘é‡åšä¸€ä¸ªå˜æ¢ï¼Œä½¿å¾—å˜æ¢åçš„Queryå’ŒKeyå¸¦æœ‰ä½ç½®ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥ä½¿å¾—Attentionçš„å†…ç§¯æ“ä½œä¸éœ€è¦åšä»»ä½•æ›´æ”¹å°±èƒ½è‡ªåŠ¨æ„ŸçŸ¥åˆ°ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚

æ¢å¥è¯è¯´ï¼Œ**RoPRçš„å‡ºå‘ç‚¹å’Œç­–ç•¥ç”¨çš„ç›¸å¯¹ä½ç½®ç¼–ç æ€æƒ³ï¼Œä½†æ˜¯å®ç°æ–¹å¼çš„ç¡®ç”¨çš„æ˜¯ç»å¯¹ä½ç½®ç¼–ç ã€‚**

è®ºæ–‡ä¸­æœ‰ä¸ªå¾ˆç›´è§‚çš„å›¾ç‰‡å±•ç¤ºäº†æ—‹è½¬å˜æ¢çš„è¿‡ç¨‹ï¼š

![](https://segmentfault.com/img/remote/1460000045501348)

æˆ‘ä»¬å¯ä»¥ä»æ—‹è½¬çŸ©é˜µçš„è§’åº¦ç†è§£RoPEï¼š

è¡¥å……ä¸€ä¸ªæœ€é‡è¦çš„ï¼šè½¬ç½®çš„æ€§è´¨ï¼Œ$ (AB)^T=B^TA^T$

![](https://segmentfault.com/img/remote/1460000045501349)

**RoPEä»£ç å®ç°ï¼š**

```python
class LlamaRotaryEmbedding(torch.nn.Module):
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))
        self.register_buffer("inv_freq", inv_freq)
        # Build here to make `torch.jit.trace` work.
        self.max_seq_len_cached = max_position_embeddings
        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device,
                         dtype=self.inv_freq.dtype)
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        emb = torch.cat((freqs, freqs), dim=-1)
        dtype = torch.get_default_dtype()
        self.register_buffer("cos_cached", emb.cos()[None, None, :, :].to(dtype), persistent=False)
        self.register_buffer("sin_cached", emb.sin()[None, None, :, :].to(dtype), persistent=False)
  

    def forward(self, x, seq_len=None):
        # x: [bs, num_attention_heads, seq_len, head_size]
        # This `if` block is unlikely to be run after we build sin/cos in `__init__`.
        # Keep the logic here just in case.
        if seq_len > self.max_seq_len_cached:
            self.max_seq_len_cached = seq_len
        t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # Different from paper, but it uses a different permutation
        # in order to obtain the same calculation
        emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
        self.register_buffer("cos_cached", emb.cos()[None, None, :, :].to(x.dtype),
                             persistent=False)
        self.register_buffer("sin_cached", emb.sin()[None, None, :, :].to(x.dtype),
                             persistent=False)
        return (
            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),
  
    # ç”Ÿæˆæ—‹è½¬çŸ©é˜µ  
    def rotate_half(x):
        """Rotates half the hidden dims of the input."""
        x1 = x[..., : x.shape[-1] // 2]
        x2 = x[..., x.shape[-1] // 2:]
        return torch.cat((-x2, x1), dim=-1)


    # æ—‹è½¬ä½ç½®ç¼–ç è®¡ç®—
    def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
        # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.
        cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]
        sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]
        cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
        sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
        q_embed = (q * cos) + (rotate_half(q) * sin)
        k_embed = (k * cos) + (rotate_half(k) * sin)
        return q_embed, k_embed
```

```python

class Attention(nn.Module):

    def __init__(self, args: ModelArgs):
        super().__init__()

        self.wq = Linear(...)
        self.wk = Linear(...)
        self.wv = Linear(...)
  
        self.freqs_cis = precompute_freqs_cis(dim, max_seq_len * 2)

    def forward(self, x: torch.Tensor):
        bsz, seqlen, _ = x.shape
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

        xq = xq.view(batch_size, seq_len, dim)
        xk = xk.view(batch_size, seq_len, dim)
        xv = xv.view(batch_size, seq_len, dim)

        # attention æ“ä½œä¹‹å‰ï¼ŒQueryã€Keyè®¡ç®—ä¹‹åï¼Œåº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆä¸ä¼šå¼•å…¥å™ªå£°ï¼‰
        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
  
        # scores.shape = (bs, seqlen, seqlen)
        scores = torch.matmul(xq, xk.transpose(1, 2)) / math.sqrt(dim)
        scores = F.softmax(scores.float(), dim=-1)
        output = torch.matmul(scores, xv)  # (batch_size, seq_len, dim)
   
     # ......
```

æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰é€šè¿‡æ—‹è½¬çŸ©é˜µå°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°å‘é‡ä¸­ï¼Œå¹¶é€šè¿‡çŸ©é˜µçš„ç»„åˆï¼Œä½¿å¾—æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿè‡ªç„¶åœ°è€ƒè™‘åˆ°ç›¸å¯¹ä½ç½®çš„ä¿¡æ¯ã€‚è€Œâ€œå†…ç§¯çš„çº¿æ€§å åŠ æ€§â€æŒ‡çš„æ˜¯å†…ç§¯å¯¹äºå‘é‡åŠ æ³•æ˜¯çº¿æ€§çš„ï¼Œè¿™ç¡®ä¿äº†å³ä½¿è¿›è¡Œæ—‹è½¬ï¼Œå‘é‡ä¹‹é—´çš„ç›¸å¯¹å…³ç³»ä¸ä¼šè¢«ç ´åã€‚
